{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Sensor Data with Spark Streaming\n",
    "\n",
    "First of all, Spark Streaming requires more than one executor, in my case I am using a VM and needed to make sure this VM had more than virtual processor at least.\n",
    "\n",
    "In the case of VirtualBox this is pretty easy of configure, just select your virtual machine and then settins -> system -> processor -> and select your desired number of virtual processors in the slider (VirtualBox 6.0) , I am currently using two virtual processors as I think I won't need more in this quick demo.\n",
    "\n",
    "Now we can work on getting the sensor data we will be taking data from a weather sensor provided by the HPWREN, an interdisciplinary and multi-institutional UCSD research and education project. In this demo we would like to return the average wind direction (called Dm in the output of the sensor), before using the sensor we have studied the values it generates and have generated a function that parses each line and returns the average wind direction as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse(line):\n",
    "    match = re.search(\"Dm=(\\d+)\", line)\n",
    "    if match:\n",
    "        val = match.group(1)\n",
    "        return [int(val)]\n",
    "    return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to analyze our sensor data in real time we will need Spark Streaming, now we will import and create a new instance of Spark's StremingContext. Similarly to the SparkContext, the StreamingContext provides an interface to Spark's streaming capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "#The argument sc is the SparkContext and 1 specifies a batch interval of one second\n",
    "ssc = StreamingContext(sc, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a Dstream of weather data, micro batches (Dstream is short for discretized streams) by opening a connection to the streaming weather data sensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = ssc.socketTextStream(\"rtd.hpwren.ucsd.edu\", 12028)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will read the average wind speed from each line and store it in a new DStream variable, then we will use **flatMap transformation** parsing the input lines in order to return an RDD with an aggregate of all the results for all the elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "values = lines.flatMap(parse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this moment, we can create a sliding window over the measurements by calling the **window()** method, this will create a new DStream window that combines 'x' seconds worth of data and moves by 'y', similar to the way of TCP works using sliding windows too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "window = values.window(10, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define and call analysis function\n",
    "\n",
    "We will find the minimum and maximum values in our window. Next we have a function which can do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stats(rdd):\n",
    "    print(rdd.collect())\n",
    "    if rdd.count() > 0:\n",
    "        print(\"max = {}, min = {}\".format(rdd.max(), rdd.min()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function first prints the entire contents of the RDD by calling the **collect()** method. This is done to demonstrate the sliding window and would not be practical if the RDD was containing a large amount of data. Next, we check if the size of the RDD is greater than zero before printing the maximum and minimum values.\n",
    "\n",
    "Next, we call the **stats()** function for each RDD in our sliding window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "window.foreachRDD(lambda rdd: stats(rdd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we are prepared to start the stream processing calling **start()** on the StreamingContext:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 19]\n",
      "max = 19, min = 16\n",
      "[16, 19, 16, 13, 12, 5, 356]\n",
      "max = 356, min = 5\n",
      "[16, 13, 12, 5, 356, 347, 340, 334, 337, 337]\n",
      "max = 356, min = 5\n",
      "[347, 340, 334, 337, 337, 344, 344, 344, 347, 356]\n",
      "max = 356, min = 334\n",
      "[344, 344, 344, 347, 356, 2, 4, 4, 3, 6, 9]\n",
      "max = 356, min = 2\n",
      "[2, 4, 4, 3, 6, 9, 9, 10, 4, 7, 7]\n",
      "max = 10, min = 2\n",
      "[9, 10, 4, 7, 7, 10, 15, 18, 12, 3]\n",
      "max = 18, min = 3\n",
      "[10, 15, 18, 12, 3, 3, 1, 7, 1, 352]\n",
      "max = 352, min = 1\n",
      "[3, 1, 7, 1, 352, 350, 344, 344, 344, 348]\n",
      "max = 352, min = 1\n"
     ]
    }
   ],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sliding window contains ten seconds worth of data and slides every five seconds. In the beginning, the number of values in the windows are incresing as the data accumulates, and after window 3, the size stays approximately the same. Since the window slides half as often as the size of the window, the second half of a window becomes the first half of the next window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
