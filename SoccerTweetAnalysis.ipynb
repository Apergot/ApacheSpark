{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soccer tweets analysis about countries\n",
    "\n",
    "In this exercise we will take a csv directly exported from a Mongodb collection, this collection holds data about twitter users who has been tweeting about soccer.\n",
    "\n",
    "First of all, we have a soccer_tweets.csv which containst the \"tweet_text\" and a country_list.csv. Using both files we will get to report the countries with the most popularity on Twitter during this event. So, a good way to approach this problem would be to find which countries were mentioned the most in the tweets from our dataset and to analyze those words which are being used the most in these tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import and create a new SQLContext \n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cells we will process the countries dataset in order to get a dataframe which we can use to join with our other dataset to get all the words and how many times each word is repeated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read the country CSV file into an RDD.\n",
    "path = 'file:///home/cloudera/Downloads/big-data-3/final-project/'\n",
    "country_lines = sc.textFile(path + 'country-list.csv')\n",
    "\n",
    "country_tuples = country_lines\\\n",
    "    .map(lambda line: tuple(line.split(\",\")))\n",
    "\n",
    "countryDF = sqlContext.createDataFrame(country_tuples, [\"country\", \"code\"])\n",
    "countryDF.printSchema()\n",
    "countryDF.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do exactly the same with the tweets texts but we have to be aware that there are some empty tweets, so we will filter them in orden to avoid empty lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read tweets CSV file into RDD \n",
    "soccer_tweets= sc.textFile(path + 'soccer_tweets.csv')\n",
    "\n",
    "cleaned_soccer_tweets = soccer_tweets\\\n",
    "    .filter(lambda line: len(line)>0)\\\n",
    "    .collect()\n",
    "cleaned_soccer_tweets.pop(0) #removing header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we will use a file where we will put all the tweets content and then do the following:\n",
    "* Split each line into words and storing them in an RDD.\n",
    "* Assign an initial count value to each word by creating tuples for each word with an initial count of 1.\n",
    "* Sum all word count values by using the redyceByKey() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- word: string (nullable = true)\n",
      " |-- times: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(word='Afghanistan', times=' AFG'),\n",
       " Row(word='Albania', times=' ALB'),\n",
       " Row(word='Algeria', times=' ALG')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.remove('words.txt')\n",
    "open('words.txt', 'w+')\n",
    "with open('words.txt', 'w') as filehandle:\n",
    "    for tweet in cleaned_soccer_tweets:\n",
    "        filehandle.write('%s\\n' % tweet)\n",
    "        \n",
    "lines = sc.textFile(path + 'words.txt')\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "tuples = words.map(lambda word : (word, 1))\n",
    "words_counts = tuples.reduceByKey(lambda a, b: (a + b))\n",
    "wordsDF = sqlContext.createDataFrame(country_tuples, [\"word\", \"times\"])\n",
    "wordsDF.printSchema()\n",
    "wordsDF.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- country: string (nullable = true)\n",
      " |-- times: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(country='', times=3292),\n",
       " Row(country='https://t.co/fQftAwGAad', times=1),\n",
       " Row(country='mobile', times=1)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the DataFrame of tweet word counts\n",
    "wordsDF = sqlContext.createDataFrame(words_counts, [\"country\", \"times\"])\n",
    "wordsDF.printSchema()\n",
    "wordsDF.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have successfully count how many times appears each word and we have a dataframe which contains a list of countries, we can join both dataframes in order to accomplish what we wanted by doing this exercise, looking for countries popularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(country='Thailand', code=' THA', times=1),\n",
       " Row(country='Iceland', code=' ISL', times=2),\n",
       " Row(country='Mexico', code=' MEX', times=1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join the country and tweet DataFrames (on the appropriate column)\n",
    "merge_result = countryDF.join(wordsDF, 'country')\n",
    "merge_result.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we got a dataframe in which each row contains the country and how many times it has been mentioned in our tweets. This allow us to make some interesting queries using SparkSQL as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|sum(times)|\n",
      "+----------+\n",
      "|       397|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Total times countries where mentioned in tweets.\n",
    "from pyspark.sql.functions import sum\n",
    "merge_result.select(sum(\"times\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+\n",
      "|country|code|times|\n",
      "+-------+----+-----+\n",
      "| Norway| NOR|   52|\n",
      "|Nigeria| NGA|   49|\n",
      "| France| FRA|   42|\n",
      "+-------+----+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Top three countries by popularity\n",
    "\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "merge_result.sort(desc(\"times\")).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+\n",
      "|country|code|times|\n",
      "+-------+----+-----+\n",
      "|  Wales| WAL|   19|\n",
      "+-------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can get the times for each country\n",
    "\n",
    "merge_result.filter(merge_result[\"country\"]==\"Wales\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|       avg(times)|\n",
      "+-----------------+\n",
      "|9.022727272727273|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the average of times.\n",
    "\n",
    "from pyspark.sql.functions import mean\n",
    "\n",
    "merge_result.select(mean(\"times\")).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
